namespace: bentoml
openllm:
  defines: runnable
  containers:
    server:
      image: ghcr.io/bentoml/openllm
      entrypoint: <- `openllm start ${model} --backend ${backend} --debug --max-model-len 8464`
      resources:
        requests:
          nvidia.com/gpu: 1
  services:
    web:
      container: server
      protocol: tcp
      port: 3000
      host-port: 3000
  variables:
    model:
      type: string
      value: tiiuae/falcon-7b
    backend:
      type: string
      value: vllm
